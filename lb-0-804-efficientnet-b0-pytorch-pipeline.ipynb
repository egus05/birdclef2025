{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ae3e31",
   "metadata": {
    "papermill": {
     "duration": 0.004063,
     "end_time": "2025-06-05T08:53:46.441586",
     "exception": false,
     "start_time": "2025-06-05T08:53:46.437523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Previous work of mine and credit goes to already shared in previous notebook\n",
    "\n",
    "https://www.kaggle.com/code/kumarandatascientist/lb-0-784-efficientnet-b0-pytorch-inference\n",
    "\n",
    "what changed here \n",
    "\n",
    " HOP_LENGTH = 16\n",
    " N_MELS = 148\n",
    " FMIN = 20\n",
    " FMAX = 16000\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c4660",
   "metadata": {
    "papermill": {
     "duration": 0.002898,
     "end_time": "2025-06-05T08:53:46.448180",
     "exception": false,
     "start_time": "2025-06-05T08:53:46.445282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is exact copy of the notebook mentioned below but with TTA on Predictions\n",
    "\n",
    "- [Copied from Notebook](https://www.kaggle.com/code/kumarandatascientist/lb-0-784-efficientnet-b0-pytorch-inference)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89af45b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T08:53:46.460676Z",
     "iopub.status.busy": "2025-06-05T08:53:46.460161Z",
     "iopub.status.idle": "2025-06-05T08:54:10.110071Z",
     "shell.execute_reply": "2025-06-05T08:54:10.108601Z"
    },
    "papermill": {
     "duration": 23.660528,
     "end_time": "2025-06-05T08:54:10.112374",
     "exception": false,
     "start_time": "2025-06-05T08:53:46.451846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: cpu\n",
      "분류 체계 데이터 로드 중...\n",
      "클래스 수: 206\n",
      "사용 중인 장치: cpu\n",
      "분류 체계 데이터 로드 중...\n",
      "클래스 수: 206\n",
      "BirdCLEF-2025 추론 시작...\n",
      "TTA 활성화: True (EfficientNet 변형: 3), SEResNeXt는 내부 TTA를 가집니다.\n",
      "지정된 모든 구성에서 총 2개의 모델 파일을 찾았습니다.\n",
      "모델 로드 중: /kaggle/input/tf-efficientnet-b1-optuna/model_fold2.pth\n",
      "모델 로드 중: /kaggle/input/sedmodel/sedmodel.pth\n",
      "모델 사용: 2개 모델의 앙상블\n",
      "총 0개의 테스트 사운드스케이프를 찾았습니다.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b840c72757434086f024f8dd513cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "전체 오디오 파일 처리 중: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 데이터프레임 생성 중...\n",
      "생성된 예측값이 없습니다. 샘플 데이터를 사용하여 빈 제출 파일을 생성합니다.\n",
      "초기 제출 파일이 submission.csv에 저장되었습니다.\n",
      "제출 예측 스무딩 중...\n",
      "스무딩된 제출 파일이 submission.csv에 저장되었습니다.\n",
      "추론 완료 시간: 0.07분\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 2 # CPU 환경에서 너무 많은 워커는 오버헤드를 유발할 수 있으므로 조정\n",
    "\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv' # 올바른 경로인지 다시 확인\n",
    "\n",
    "    # 앙상블에 사용할 모든 모델 파일 경로와 해당 아키텍처 이름 (튜플 리스트)\n",
    "    # 각 튜플은 (모델 파일 경로, 모델 아키텍처 이름) 형식입니다.\n",
    "    model_configs = [\n",
    "        # SEResNeXt 모델\n",
    "        #('/kaggle/input/sedmodel/sedmodel.pth', 'seresnext26t_32x4d'),\n",
    "        # EfficientNet-B0 모델\n",
    "        ('/kaggle/input/tf-efficientnet-b1-optuna/model_fold2.pth','tf_efficientnet_b1'),\n",
    "        ('/kaggle/input/sedmodel/sedmodel.pth','seresnext26t_32x4d')\n",
    "    ]\n",
    "\n",
    "    # 오디오 파라미터 (공통)\n",
    "    SR = 32000\n",
    "    WINDOW_SIZE = 5 # EfficientNet/ResNet 세그먼트 길이\n",
    "    target_duration = 5 # SEResNeXt 예측 세그먼트 길이\n",
    "    train_duration = 10 # SEResNeXt 훈련/컨텍스트 길이\n",
    "    infer_duration = 5 # SEResNeXt 추론 시 TTA 윈도우\n",
    "\n",
    "    # 멜 스펙트로그램 파라미터 (일반적인 기본값. 모델 체크포인트 CFG로 오버라이드될 수 있음)\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    TARGET_SHAPE = (256, 256) # EfficientNet/ResNet 대상 스펙트로그램 크기\n",
    "    NORMALIZATION_MODE = 80 # SEResNeXt용 기본 정규화 모드 (top_db 80과 연관)\n",
    "\n",
    "    in_channels = 1\n",
    "    device = 'cpu' # CPU로 설정\n",
    "\n",
    "    # 추론 파라미터\n",
    "    use_tta = True # EfficientNet에 대한 TTA 활성화 여부\n",
    "    tta_count = 3 # EfficientNet TTA 변형 수 (원본, 수평 뒤집기, 수직 뒤집기)\n",
    "\n",
    "    debug = False\n",
    "    debug_count = 3 # 디버그 시 처리할 파일 개수\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "print(f\"사용 중인 장치: {cfg.device}\")\n",
    "print(f\"분류 체계 데이터 로드 중...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"클래스 수: {num_classes}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"재현성을 위해 시드를 설정합니다.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# --- SEResNeXt 모델에 필요한 보조 모듈 (이전 코드에서 가져옴) ---\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.cla = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.att.weight)\n",
    "        if hasattr(self.att, \"bias\") and self.att.bias is not None:\n",
    "            self.att.bias.data.fill_(0.0)\n",
    "        nn.init.xavier_uniform_(self.cla.weight)\n",
    "        if hasattr(self.cla, \"bias\") and self.cla.bias is not None:\n",
    "            self.cla.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "# --- 단일 BirdCLEFModel 클래스로 통합 ---\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, model_cfg, num_classes, model_arch_name):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg # 이 모델 인스턴스에 특화된 설정\n",
    "        self.num_classes = num_classes\n",
    "        self.model_arch_name = model_arch_name\n",
    "\n",
    "        self.is_seresnext = \"seresnext\" in model_arch_name.lower() # 소문자로 변환하여 비교\n",
    "\n",
    "        if self.is_seresnext:\n",
    "            self.backbone = timm.create_model(\n",
    "                model_arch_name,\n",
    "                pretrained=False,\n",
    "                in_chans=model_cfg.get('in_channels', 1),\n",
    "                drop_rate=model_cfg.get('drop_rate', 0.2), # SEResNeXt 훈련 시 드롭아웃\n",
    "                drop_path_rate=model_cfg.get('drop_path_rate', 0.2),\n",
    "            )\n",
    "            layers = list(self.backbone.children())[:-2]\n",
    "            self.encoder = nn.Sequential(*layers)\n",
    "            backbone_out = self.backbone.fc.in_features # SEResNeXt의 마지막 FC 레이어 입력 특징 수\n",
    "            self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "            self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "            self.bn0 = nn.BatchNorm2d(model_cfg.get('n_mels', cfg.N_MELS))\n",
    "\n",
    "        else: # EfficientNet 또는 ResNet\n",
    "            self.backbone = timm.create_model(\n",
    "                model_arch_name,\n",
    "                pretrained=False,\n",
    "                in_chans=model_cfg.get('in_channels', 1),\n",
    "                drop_rate=0.0, # 추론 시 드롭아웃 비활성화\n",
    "                drop_path_rate=0.0\n",
    "            )\n",
    "            if 'efficientnet' in model_arch_name:\n",
    "                backbone_out = self.backbone.classifier.in_features\n",
    "                self.backbone.classifier = nn.Identity()\n",
    "            elif 'resnet' in model_arch_name:\n",
    "                backbone_out = self.backbone.fc.in_features\n",
    "                self.backbone.fc = nn.Identity()\n",
    "            else: # 혹시 모를 다른 모델\n",
    "                backbone_out = self.backbone.get_classifier().in_features\n",
    "                self.backbone.reset_classifier(0, '')\n",
    "\n",
    "            self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "\n",
    "        # torchaudio 멜 스펙트로그램 변환 (모델별 cfg 사용)\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.model_cfg.get('SR', cfg.SR),\n",
    "            hop_length=self.model_cfg.get('hop_length', cfg.HOP_LENGTH),\n",
    "            n_mels=self.model_cfg.get('n_mels', cfg.N_MELS),\n",
    "            f_min=self.model_cfg.get('f_min', cfg.FMIN),\n",
    "            f_max=self.model_cfg.get('f_max', cfg.FMAX),\n",
    "            n_fft=self.model_cfg.get('n_fft', cfg.N_FFT),\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=model_cfg.get('top_db', 80) # top_db도 model_cfg에서 가져오거나 기본값 사용\n",
    "        )\n",
    "\n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_audio_to_spec(self, audio):\n",
    "        \"\"\"오디오를 멜 스펙트로그램으로 변환 및 정규화 (torchaudio 사용)\"\"\"\n",
    "        audio = audio.float()\n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        # 정규화 모드 (SEResNeXt 훈련 방식에 따름)\n",
    "        normal_mode = self.model_cfg.get('normal', cfg.NORMALIZATION_MODE)\n",
    "        if normal_mode == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif normal_mode == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError(f\"정규화 모드 {normal_mode}는 구현되지 않았습니다.\")\n",
    "        return spec\n",
    "\n",
    "    def extract_feature_seresnext(self, x_spec):\n",
    "        \"\"\"SEResNeXt 모델의 특징 추출 로직\"\"\"\n",
    "        # x_spec: (batch, channels, freq, time)\n",
    "        # bn0 적용을 위해 (batch, freq, time, channels) 또는 (batch, channels, time, freq)으로 변환\n",
    "        x = x_spec.permute((0, 1, 3, 2)) # (batch, channel, time, freq)\n",
    "        \n",
    "        # bn0는 (N, C, H, W)에서 C에 BatchNorm을 적용하므로, C를 freq (n_mels)로 맞춰야 합니다.\n",
    "        # 따라서 (batch, freq, time, channel)로 만들고, bn0 적용 후 원래대로 돌려놓습니다.\n",
    "        x = x.transpose(1, 3) # (batch, freq, time, channel)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3) # (batch, channel, time, freq)\n",
    "\n",
    "        x = x.transpose(2, 3) # (batch, channel, freq, time) for backbone (encoder)\n",
    "        x = self.encoder(x) # (batch_size, channels, freq, frames) -> (batch_size, channels, frames)\n",
    "\n",
    "        x = torch.mean(x, dim=2) # freq 차원 평균 풀링 (batch, channels, frames)\n",
    "\n",
    "        # 채널 스무딩\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=self.model_cfg.get('drop_rate', 0.5), training=self.training) # dropout rate도 model_cfg에서\n",
    "        x = x.transpose(1, 2) # (batch, frames, channels)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2) # (batch, channels, frames)\n",
    "        x = F.dropout(x, p=self.model_cfg.get('drop_rate', 0.5), training=self.training) # dropout rate도 model_cfg에서\n",
    "        return x\n",
    "\n",
    "    def attention_infer_seresnext(self, start, end, x_features):\n",
    "        \"\"\"SEResNeXt의 어텐션 기반 추론 로직\"\"\"\n",
    "        feat = x_features[:, :, start:end]\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0] # 클립와이즈 예측은 프레임와이즈 최대값\n",
    "        return framewise_pred_max\n",
    "\n",
    "    def infer_seresnext(self, x_audio_input, tta_delta=2):\n",
    "        \"\"\"SEResNeXt 모델의 추론 메서드 (내부 TTA 로직 포함)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x_spec = self.transform_audio_to_spec(x_audio_input)\n",
    "            x_features = self.extract_feature_seresnext(x_spec)\n",
    "            time_att = torch.tanh(self.att_block.att(x_features))\n",
    "            feat_time = x_features.size(-1)\n",
    "\n",
    "            # 원본 윈도우 예측\n",
    "            start = (feat_time / 2 - feat_time * (self.model_cfg.get('infer_duration', cfg.infer_duration) / self.model_cfg.get('train_duration', cfg.train_duration)) / 2)\n",
    "            end = start + feat_time * (self.model_cfg.get('infer_duration', cfg.infer_duration) / self.model_cfg.get('train_duration', cfg.train_duration))\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            pred = self.attention_infer_seresnext(start, end, x_features)\n",
    "\n",
    "            if cfg.use_tta: # EfficientNet TTA 플래그를 따름 (SEResNeXt도 TTA를 수행하게 함)\n",
    "                # TTA 시프트\n",
    "                start_minus = max(0, start - tta_delta)\n",
    "                end_minus = end - tta_delta\n",
    "                pred_minus = self.attention_infer_seresnext(start_minus, end_minus, x_features)\n",
    "\n",
    "                start_plus = start + tta_delta\n",
    "                end_plus = min(feat_time, end + tta_delta)\n",
    "                pred_plus = self.attention_infer_seresnext(start_plus, end_plus, x_features)\n",
    "                final_pred = 0.5 * pred + 0.25 * pred_minus + 0.25 * pred_plus\n",
    "            else:\n",
    "                final_pred = pred\n",
    "            return final_pred\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \"\"\"\n",
    "        모델 아키텍처에 따라 포워드 패스를 수행합니다.\n",
    "        EfficientNet/ResNet: 멜 스펙트로그램(이미지) 입력.\n",
    "        SEResNeXt: 원본 오디오 입력 (추론 시 infer_seresnext 사용 권장).\n",
    "        \"\"\"\n",
    "        if self.is_seresnext:\n",
    "            # 이 forward는 훈련 시 사용될 수 있습니다. 추론 시에는 `infer_seresnext`를 직접 호출하세요.\n",
    "            # 하지만 혹시 이곳으로 들어온다면 (예: 테스트 코드에서 직접 호출)\n",
    "            # x_input이 원본 오디오라고 가정하고 멜 스펙트로그램 변환을 수행합니다.\n",
    "            x_spec = self.transform_audio_to_spec(x_input)\n",
    "            x_features = self.extract_feature_seresnext(x_spec)\n",
    "            clipwise_output, _, _ = self.att_block(x_features)\n",
    "            return torch.logit(clipwise_output)\n",
    "        else: # EfficientNet/ResNet\n",
    "            features = self.backbone(x_input) # x_input은 이미 멜 스펙트로그램\n",
    "\n",
    "            if isinstance(features, dict):\n",
    "                features = features['features']\n",
    "\n",
    "            if len(features.shape) == 4: # CNN 백본 출력\n",
    "                features = self.pooling(features)\n",
    "                features = features.view(features.size(0), -1)\n",
    "            \n",
    "            logits = self.classifier(features)\n",
    "            return logits\n",
    "\n",
    "\n",
    "# --- 파이프라인 클래스 ---\n",
    "class BirdCLEF2025Pipeline:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.taxonomy_df = None\n",
    "        self.species_ids = []\n",
    "        self.models = [] # 모든 모델을 담을 리스트 (SEResNeXt, EfficientNet 등)\n",
    "        self._load_taxonomy()\n",
    "\n",
    "    def _load_taxonomy(self):\n",
    "        print(\"분류 체계 데이터 로드 중...\")\n",
    "        self.taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        self.species_ids = self.taxonomy_df['primary_label'].tolist()\n",
    "        print(f\"클래스 수: {len(self.species_ids)}\")\n",
    "\n",
    "    def audio2melspec_librosa(self, audio_data, custom_cfg):\n",
    "        \"\"\"\n",
    "        librosa를 사용하여 오디오 데이터를 멜 스펙트로그램으로 변환합니다.\n",
    "        EfficientNet/ResNet 모델에 사용됩니다.\n",
    "        \"\"\"\n",
    "        if np.isnan(audio_data).any():\n",
    "            mean_signal = np.nanmean(audio_data)\n",
    "            audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio_data,\n",
    "            sr=custom_cfg.get('SR', self.cfg.SR), # cfg에서 SR 가져오기\n",
    "            n_fft=custom_cfg.get('N_FFT', self.cfg.N_FFT),\n",
    "            hop_length=custom_cfg.get('hop_length', self.cfg.HOP_LENGTH),\n",
    "            n_mels=custom_cfg.get('n_mels', self.cfg.N_MELS),\n",
    "            fmin=custom_cfg.get('f_min', self.cfg.FMIN),\n",
    "            fmax=custom_cfg.get('f_max', self.cfg.FMAX),\n",
    "            power=2.0\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "        return mel_spec_norm\n",
    "\n",
    "    def process_audio_segment_for_cnn_model(self, audio_data, custom_cfg):\n",
    "        \"\"\"\n",
    "        EfficientNet/ResNet과 같은 CNN 모델을 위해 오디오 세그먼트를 멜 스펙트로그램으로 처리합니다.\n",
    "        \"\"\"\n",
    "        segment_len_samples = custom_cfg.get('SR', self.cfg.SR) * custom_cfg.get('WINDOW_SIZE', self.cfg.WINDOW_SIZE)\n",
    "        if len(audio_data) < segment_len_samples:\n",
    "            audio_data = np.pad(\n",
    "                audio_data,\n",
    "                (0, segment_len_samples - len(audio_data)),\n",
    "                mode='constant'\n",
    "            )\n",
    "            \n",
    "        mel_spec = self.audio2melspec_librosa(audio_data, custom_cfg)\n",
    "        \n",
    "        target_shape = custom_cfg.get('TARGET_SHAPE', self.cfg.TARGET_SHAPE)\n",
    "        if mel_spec.shape != target_shape:\n",
    "            # cv2.resize는 (width, height)를 기대하므로, (TARGET_SHAPE[1], TARGET_SHAPE[0]) 순서로 전달\n",
    "            mel_spec = cv2.resize(mel_spec, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "        return mel_spec.astype(np.float32)\n",
    "\n",
    "    def apply_tta_cnn(self, spec, tta_idx):\n",
    "        \"\"\"EfficientNet/ResNet 모델을 위한 테스트 시간 증강을 적용합니다.\"\"\"\n",
    "        if tta_idx == 0:\n",
    "            return spec\n",
    "        elif tta_idx == 1: # 수평 뒤집기 (시간 축)\n",
    "            return np.flip(spec, axis=1).copy()\n",
    "        elif tta_idx == 2: # 수직 뒤집기 (주파수 축)\n",
    "            return np.flip(spec, axis=0).copy()\n",
    "        else:\n",
    "            return spec\n",
    "\n",
    "    def find_all_model_files(self):\n",
    "        \"\"\"\n",
    "        `cfg.model_configs`에 명시된 모든 모델 파일 경로를 반환합니다.\n",
    "        \"\"\"\n",
    "        # CFG.model_configs는 (path, name) 튜플 리스트이므로, 경로만 추출합니다.\n",
    "        return [config[0] for config in self.cfg.model_configs]\n",
    "\n",
    "    def load_models(self):\n",
    "        self.models = []\n",
    "        all_model_configs = self.cfg.model_configs\n",
    "        \n",
    "        if not all_model_configs:\n",
    "            print(\"경고: 지정된 모델 구성이 없습니다!\")\n",
    "            return self.models\n",
    "\n",
    "        print(f\"지정된 모든 구성에서 총 {len(all_model_configs)}개의 모델 파일을 찾았습니다.\")\n",
    "        \n",
    "        for model_path, hardcoded_model_arch_name in all_model_configs:\n",
    "            try:\n",
    "                print(f\"모델 로드 중: {model_path}\")\n",
    "                checkpoint = torch.load(model_path, map_location=torch.device(self.cfg.device))\n",
    "                \n",
    "                model_cfg_for_instance = None\n",
    "                final_model_arch_name = hardcoded_model_arch_name\n",
    "\n",
    "                if 'cfg' in checkpoint:\n",
    "                    loaded_cfg_data = checkpoint['cfg']\n",
    "                    if isinstance(loaded_cfg_data, dict):\n",
    "                        model_cfg_for_instance = loaded_cfg_data\n",
    "                    elif hasattr(loaded_cfg_data, '__dict__'):\n",
    "                        model_cfg_for_instance = loaded_cfg_data.__dict__\n",
    "                \n",
    "                if not model_cfg_for_instance:\n",
    "                    model_cfg_for_instance = self.cfg.__dict__.copy()\n",
    "                \n",
    "                model_cfg_for_instance.update({\n",
    "                    'model_name': final_model_arch_name,\n",
    "                    'device': self.cfg.device,\n",
    "                    'in_channels': self.cfg.in_channels,\n",
    "                    'SR': self.cfg.SR,\n",
    "                    'target_duration': self.cfg.target_duration,\n",
    "                    'train_duration': self.cfg.train_duration,\n",
    "                    'infer_duration': self.cfg.infer_duration,\n",
    "                    'N_FFT': self.cfg.N_FFT,\n",
    "                    'HOP_LENGTH': self.cfg.HOP_LENGTH,\n",
    "                    'N_MELS': self.cfg.N_MELS,\n",
    "                    'FMIN': self.cfg.FMIN,\n",
    "                    'FMAX': self.cfg.FMAX,\n",
    "                    'TARGET_SHAPE': self.cfg.TARGET_SHAPE,\n",
    "                    'WINDOW_SIZE': self.cfg.WINDOW_SIZE,\n",
    "                    'NORMALIZATION_MODE': self.cfg.NORMALIZATION_MODE\n",
    "                })\n",
    "                \n",
    "                if 'efficientnet' in final_model_arch_name.lower():\n",
    "                    model_cfg_for_instance['drop_rate'] = 0.0\n",
    "                    model_cfg_for_instance['drop_path_rate'] = 0.0\n",
    "\n",
    "\n",
    "                model = BirdCLEFModel(model_cfg_for_instance, len(self.species_ids), final_model_arch_name)\n",
    "                \n",
    "                if 'model_state_dict' not in checkpoint or not checkpoint['model_state_dict']:\n",
    "                    print(f\"경고: {model_path}에서 'model_state_dict'를 찾을 수 없거나 비어 있습니다. 건너뜁니다.\")\n",
    "                    continue\n",
    "\n",
    "                # --- 핵심 수정: strict=False 옵션 추가 ---\n",
    "                # SEResNeXt 모델은 melspec_transform 및 db_transform 가중치를 포함할 수 있으므로,\n",
    "                # SEResNeXt만 strict=True를 유지하고 EfficientNet은 strict=False로 설정하는 것이 더 안전할 수 있습니다.\n",
    "                # 그러나 둘 다 Missing Keys가 있다면, 모든 모델에 strict=False를 적용합니다.\n",
    "                if model.is_seresnext: # SEResNeXt 모델인 경우\n",
    "                    # SEResNeXt 체크포인트가 모든 melspec_transform 키를 가지고 있다면 strict=True\n",
    "                    # 그렇지 않다면 strict=False\n",
    "                    # 현재 로그를 보면 EfficientNet에서 Missing keys가 발생했으니, SEResNeXt는 괜찮을 수 있음.\n",
    "                    # 일단 모든 모델에 strict=False를 적용하여 오류를 해결하고 진행합시다.\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "                else: # EfficientNet 또는 ResNet 모델인 경우\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "                model = model.to(self.cfg.device)\n",
    "                model.eval()\n",
    "\n",
    "                # torchaudio transform은 모델이 로드된 후에 같은 디바이스에 올립니다.\n",
    "                # 모델 아키텍처에 포함되어 있으므로 이 부분은 유지.\n",
    "                model.melspec_transform = model.melspec_transform.to(self.cfg.device)\n",
    "                model.db_transform = model.db_transform.to(self.cfg.device)\n",
    "\n",
    "                if \"seresnext\" in final_model_arch_name.lower():\n",
    "                    model.half().float()\n",
    "\n",
    "                self.models.append(model)\n",
    "            except Exception as e:\n",
    "                print(f\"모델 {model_path} ({final_model_arch_name if 'final_model_arch_name' in locals() else 'unknown'}) 로드 오류: {e}\")\n",
    "        \n",
    "        return self.models\n",
    "\n",
    "    def predict_single_segment_ensemble(self, audio_data_full, segment_idx, soundscape_id):\n",
    "        \"\"\"\n",
    "        단일 5초 오디오 세그먼트에 대해 모든 앙상블 모델을 사용하여 예측을 수행합니다.\n",
    "        `run_inference`에서 병렬 처리될 각 오디오 파일에 대해 호출됩니다.\n",
    "        \"\"\"\n",
    "        # 모델별 예측값과 해당 가중치를 저장할 리스트\n",
    "        individual_model_preds = []\n",
    "        applied_weights = [] # 각 모델 예측에 적용할 가중치 리스트\n",
    "\n",
    "        # 5초 창에 대한 세그먼트 경계 계산\n",
    "        segment_length_samples = self.cfg.SR * self.cfg.WINDOW_SIZE\n",
    "        start_sample_5sec = segment_idx * segment_length_samples\n",
    "        end_sample_5sec = min(len(audio_data_full), start_sample_5sec + segment_length_samples)\n",
    "        current_5sec_audio = audio_data_full[start_sample_5sec:end_sample_5sec]\n",
    "\n",
    "        # 5초 길이가 안되면 0으로 채움\n",
    "        if len(current_5sec_audio) < segment_length_samples:\n",
    "            current_5sec_audio = np.pad(current_5sec_audio,\n",
    "                                        (0, segment_length_samples - len(current_5sec_audio)),\n",
    "                                        mode='constant')\n",
    "\n",
    "        # 모델별 가중치 정의\n",
    "        # 현재 CFG.model_configs: ('tf_efficientnet_b1'), ('seresnext26t_32x4d')\n",
    "        # EfficientNetB1 1개와 SEResNeXt 1개에 대한 가중치\n",
    "        WEIGHT_SERESNEXT = 0.8\n",
    "        WEIGHT_EFFICIENTNET = 0.2\n",
    "\n",
    "        for model in self.models:\n",
    "            model_type = model.model_arch_name.lower()\n",
    "            \n",
    "            # 예측값 초기화\n",
    "            probs = None\n",
    "\n",
    "            if 'seresnext' in model_type:\n",
    "                # SEResNeXt 모델은 10초 컨텍스트를 기대하므로 5초 오디오를 10초로 확장합니다.\n",
    "                expanded_audio_segment = np.zeros(self.cfg.SR * self.cfg.train_duration, dtype=np.float32)\n",
    "                start_fill_idx = (self.cfg.SR * self.cfg.train_duration - len(current_5sec_audio)) // 2\n",
    "                expanded_audio_segment[start_fill_idx : start_fill_idx + len(current_5sec_audio)] = current_5sec_audio\n",
    "\n",
    "                audio_tensor = torch.tensor(expanded_audio_segment, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.cfg.device)\n",
    "                probs = model.infer_seresnext(audio_tensor, tta_delta=2).cpu().numpy().squeeze()\n",
    "                \n",
    "                # SEResNeXt에 가중치 할당\n",
    "                applied_weights.append(WEIGHT_SERESNEXT)\n",
    "            else: # EfficientNet, ResNet (현재 구성상 EfficientNetB1)\n",
    "                # EfficientNet/ResNet은 5초 멜 스펙트로그램 입력\n",
    "                mel_spec = self.process_audio_segment_for_cnn_model(current_5sec_audio, model.model_cfg) # 모델별 cfg 사용\n",
    "\n",
    "                if self.cfg.use_tta:\n",
    "                    tta_preds_for_model = []\n",
    "                    for tta_idx in range(self.cfg.tta_count):\n",
    "                        augmented_mel_spec = self.apply_tta_cnn(mel_spec, tta_idx)\n",
    "                        mel_spec_tensor = torch.tensor(augmented_mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.cfg.device)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec_tensor)\n",
    "                            probs_tta = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                        tta_preds_for_model.append(probs_tta)\n",
    "                    probs = np.mean(tta_preds_for_model, axis=0) # EfficientNet TTA는 여전히 단순 평균\n",
    "                else:\n",
    "                    mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.cfg.device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(mel_spec_tensor)\n",
    "                        probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                \n",
    "                # EfficientNet에 가중치 할당\n",
    "                applied_weights.append(WEIGHT_EFFICIENTNET)\n",
    "            \n",
    "            # 예측값이 계산되었으면 리스트에 추가\n",
    "            if probs is not None:\n",
    "                individual_model_preds.append(probs)\n",
    "            else:\n",
    "                # 예상치 못한 경우 (모델 타입이 SEResNeXt도 EfficientNet도 아님)\n",
    "                print(f\"경고: 알 수 없는 모델 타입 ({model_type})이 감지되었습니다. 해당 모델의 예측값은 0으로 처리됩니다.\")\n",
    "                individual_model_preds.append(np.zeros(len(self.species_ids)))\n",
    "                # 가중치 리스트의 길이를 맞추기 위해 0을 추가 (가중치 0과 동일)\n",
    "                applied_weights.append(0.0) \n",
    "\n",
    "        # 모든 앙상블 모델의 예측을 가중 평균\n",
    "        # np.average 함수를 사용하여 가중치 적용\n",
    "        if not individual_model_preds:\n",
    "            # 예측 리스트가 비어있다면, 0으로 채워진 배열 반환 (오류 방지)\n",
    "            final_ensemble_pred = np.zeros(len(self.species_ids))\n",
    "        else:\n",
    "            final_ensemble_pred = np.average(individual_model_preds, axis=0, weights=applied_weights)\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * self.cfg.WINDOW_SIZE\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        return row_id, final_ensemble_pred\n",
    "\n",
    "    def run_inference(self):\n",
    "        \"\"\"모든 테스트 사운드스케이프에 대해 앙상블 추론을 실행합니다.\"\"\"\n",
    "        test_files = list(Path(self.cfg.test_soundscapes).glob('*.ogg'))\n",
    "        \n",
    "        if self.cfg.debug:\n",
    "            print(f\"디버그 모드 활성화됨, {self.cfg.debug_count}개 파일만 사용\")\n",
    "            test_files = test_files[:self.cfg.debug_count]\n",
    "                \n",
    "        print(f\"총 {len(test_files)}개의 테스트 사운드스케이프를 찾았습니다.\")\n",
    "\n",
    "        all_row_ids = []\n",
    "        all_predictions = []\n",
    "\n",
    "        # 각 오디오 파일에 대한 처리 함수\n",
    "        def process_audio_file(audio_path_str):\n",
    "            audio_path = Path(audio_path_str)\n",
    "            soundscape_id = audio_path.stem\n",
    "            local_row_ids = []\n",
    "            local_predictions = []\n",
    "            \n",
    "            try:\n",
    "                print(f\"Processing {soundscape_id}\")\n",
    "                audio_data_full, _ = librosa.load(audio_path_str, sr=self.cfg.SR, mono=True)\n",
    "                total_segments = math.ceil(len(audio_data_full) / (self.cfg.SR * self.cfg.WINDOW_SIZE))\n",
    "\n",
    "                for segment_idx in range(total_segments):\n",
    "                    row_id, prediction = self.predict_single_segment_ensemble(\n",
    "                        audio_data_full, segment_idx, soundscape_id\n",
    "                    )\n",
    "                    local_row_ids.append(row_id)\n",
    "                    local_predictions.append(prediction)\n",
    "            except Exception as e:\n",
    "                print(f\"파일 {audio_path_str} 처리 오류: {e}\")\n",
    "                # 오류 발생 시 해당 사운드스케이프의 모든 세그먼트는 0으로 예측\n",
    "                # 샘플 제출 파일의 첫 번째 row_id를 복사하여 사용 (길이 맞추기 위함)\n",
    "                sample_sub = pd.read_csv(self.cfg.submission_csv)\n",
    "                if not sample_sub.empty:\n",
    "                    # sample_sub에 있는 row_id 패턴을 활용하여 더미 row_id 생성\n",
    "                    dummy_row_pattern = f\"{soundscape_id}_\"\n",
    "                    # 5초 간격으로 끝나는 row_id를 예상하므로 해당 패턴에 맞춰 생성\n",
    "                    for seg_idx in range(math.ceil(len(audio_data_full) / (self.cfg.SR * self.cfg.WINDOW_SIZE))):\n",
    "                        local_row_ids.append(f\"{dummy_row_pattern}{(seg_idx + 1) * self.cfg.WINDOW_SIZE}\")\n",
    "                        local_predictions.append(np.zeros(len(self.species_ids)))\n",
    "                else: # 샘플 제출 파일이 비어있는 경우\n",
    "                    local_row_ids.append(f\"{soundscape_id}_ERROR_0\")\n",
    "                    local_predictions.append(np.zeros(len(self.species_ids)))\n",
    "\n",
    "            return local_row_ids, local_predictions\n",
    "        \n",
    "        # ThreadPoolExecutor를 사용하여 오디오 파일 병렬 처리\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.cfg.num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_audio_file, test_files), total=len(test_files), desc=\"전체 오디오 파일 처리 중\"))\n",
    "\n",
    "        for rids, preds in results:\n",
    "            all_row_ids.extend(rids)\n",
    "            all_predictions.extend(preds)\n",
    "                \n",
    "        return all_row_ids, all_predictions\n",
    "\n",
    "    def create_submission(self, row_ids, predictions):\n",
    "        \"\"\"제출 데이터프레임을 생성합니다.\"\"\"\n",
    "        print(\"제출 데이터프레임 생성 중...\")\n",
    "        \n",
    "        # 예측이 비어있지 않은지 확인\n",
    "        if not predictions:\n",
    "            print(\"생성된 예측값이 없습니다. 샘플 데이터를 사용하여 빈 제출 파일을 생성합니다.\")\n",
    "            submission_df = pd.read_csv(self.cfg.submission_csv)\n",
    "            submission_df[submission_df.columns[1:]] = 0.0 \n",
    "            return submission_df\n",
    "            \n",
    "        submission_dict = {'row_id': row_ids}\n",
    "        for i, species in enumerate(self.species_ids):\n",
    "            submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "        submission_df = pd.DataFrame(submission_dict)\n",
    "        submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "        sample_sub = pd.read_csv(self.cfg.submission_csv, index_col='row_id')\n",
    "        missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"경고: 제출 파일에 {len(missing_cols)}개의 종 컬럼이 누락되었습니다.\")\n",
    "            for col in missing_cols:\n",
    "                submission_df[col] = 0.0\n",
    "\n",
    "        # 샘플 서브미션의 컬럼 순서와 일치시키기\n",
    "        submission_df = submission_df[sample_sub.columns]\n",
    "        submission_df = submission_df.reset_index()\n",
    "        \n",
    "        return submission_df\n",
    "\n",
    "    def smooth_submission(self, submission_path):\n",
    "        \"\"\"\n",
    "        제출 CSV를 후처리하여 예측을 스무딩하여 시간적 일관성을 강화합니다.\n",
    "        \"\"\"\n",
    "        print(\"제출 예측 스무딩 중...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        \n",
    "        if sub.empty:\n",
    "            print(\"제출 데이터프레임이 비어 있어 스무딩을 건너뜁니다.\")\n",
    "            return\n",
    "\n",
    "        cols = sub.columns[1:]\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # 이웃 세그먼트를 사용하여 예측 스무딩 (SEResNeXt 솔루션의 가중치 사용)\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + \\\n",
    "                                         (predictions[i] * 0.6) + \\\n",
    "                                         (predictions[i+1] * 0.2)\n",
    "            \n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"스무딩된 제출 파일이 {submission_path}에 저장되었습니다.\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"전체 추론 파이프라인을 실행하는 메인 메서드입니다.\"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"BirdCLEF-2025 추론 시작...\")\n",
    "        print(f\"TTA 활성화: {self.cfg.use_tta} (EfficientNet 변형: {self.cfg.tta_count if self.cfg.use_tta else 0}), SEResNeXt는 내부 TTA를 가집니다.\")\n",
    "        \n",
    "        self.load_models()\n",
    "        if not self.models:\n",
    "            print(\"모델을 찾을 수 없습니다! 모델 경로를 확인해주세요. 종료합니다.\")\n",
    "            # 모델이 없으면 빈 제출 파일 생성\n",
    "            empty_submission_df = self.create_submission([], [])\n",
    "            empty_submission_df.to_csv('submission.csv', index=False)\n",
    "            print(\"모델을 찾을 수 없어 빈 submission.csv를 생성했습니다.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"모델 사용: {len(self.models)}개 모델의 앙상블\")\n",
    "        row_ids, predictions = self.run_inference()\n",
    "        submission_df = self.create_submission(row_ids, predictions)\n",
    "        \n",
    "        submission_path = 'submission.csv'\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        print(f\"초기 제출 파일이 {submission_path}에 저장되었습니다.\")\n",
    "        \n",
    "        self.smooth_submission(submission_path)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"추론 완료 시간: {(end_time - start_time) / 60:.2f}분\")\n",
    "\n",
    "# --- 파이프라인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CFG()\n",
    "    print(f\"사용 중인 장치: {cfg.device}\")\n",
    "    pipeline = BirdCLEF2025Pipeline(cfg)\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c54fc4",
   "metadata": {
    "papermill": {
     "duration": 0.005587,
     "end_time": "2025-06-05T08:54:10.122962",
     "exception": false,
     "start_time": "2025-06-05T08:54:10.117375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2466d3",
   "metadata": {
    "papermill": {
     "duration": 0.004058,
     "end_time": "2025-06-05T08:54:10.131658",
     "exception": false,
     "start_time": "2025-06-05T08:54:10.127600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa602c",
   "metadata": {
    "papermill": {
     "duration": 0.003675,
     "end_time": "2025-06-05T08:54:10.139917",
     "exception": false,
     "start_time": "2025-06-05T08:54:10.136242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7467916,
     "sourceId": 11882304,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7478695,
     "sourceId": 11897412,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7481868,
     "sourceId": 11902130,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7482680,
     "sourceId": 11903470,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7483544,
     "sourceId": 11904706,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7508718,
     "sourceId": 11944147,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7520424,
     "sourceId": 11960260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7578288,
     "sourceId": 12043021,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.302995,
   "end_time": "2025-06-05T08:54:12.765587",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T08:53:42.462592",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00d9f4ce9a6d4598b767fd92b73f3b73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "24b840c72757434086f024f8dd513cef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9d62165559154b2fa4b10a1469547631",
        "IPY_MODEL_b1e936a4813b4f08a1713adccb0e4e6c",
        "IPY_MODEL_62f0447934f84169b7b318253d13eb71"
       ],
       "layout": "IPY_MODEL_366777f8b3604f2baae4c3d86c1218e5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "366777f8b3604f2baae4c3d86c1218e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62f0447934f84169b7b318253d13eb71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b99609b3e6e94850b392e8c6cc42cdf7",
       "placeholder": "​",
       "style": "IPY_MODEL_00d9f4ce9a6d4598b767fd92b73f3b73",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "9d62165559154b2fa4b10a1469547631": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c791180288d54fae8841787227155fc4",
       "placeholder": "​",
       "style": "IPY_MODEL_e544d6fd0a084f23a44bc32f73b342f9",
       "tabbable": null,
       "tooltip": null,
       "value": "전체 오디오 파일 처리 중: "
      }
     },
     "a34ca082341a4604aa567e4bc6686a63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "b1e936a4813b4f08a1713adccb0e4e6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a34ca082341a4604aa567e4bc6686a63",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3eef693cb624e91a4683b84e3b39635",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "b3eef693cb624e91a4683b84e3b39635": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b99609b3e6e94850b392e8c6cc42cdf7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c791180288d54fae8841787227155fc4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e544d6fd0a084f23a44bc32f73b342f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
